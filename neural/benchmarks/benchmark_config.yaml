# Neural DSL Benchmark Configuration
# 
# This file defines comprehensive benchmark scenarios for comparing
# Neural DSL against competing frameworks.

# Global benchmark settings
global:
  output_dir: benchmark_results
  report_dir: benchmark_reports
  num_inference_samples: 100
  warmup_runs: 2
  measurement_runs: 10

# Framework selection
# Uncomment to enable specific frameworks
frameworks:
  - neural_dsl
  - keras
  - raw_tensorflow
  - pytorch_lightning
  - raw_pytorch
  # - fastai  # Optional: requires fastai package
  # - ludwig  # Optional: requires ludwig package (slower)

# Benchmark tasks
# Each task represents a different model architecture and dataset
tasks:
  # Simple CNN for MNIST
  - name: MNIST_Simple_CNN
    description: "Basic CNN for handwritten digit classification"
    dataset: mnist
    model_type: simple_cnn
    epochs: 5
    batch_size: 32
    expected_accuracy: 0.96
    priority: high

  # Medium complexity ResNet-style model
  - name: MNIST_ResNet_Style
    description: "ResNet-inspired architecture for MNIST"
    dataset: mnist
    model_type: resnet
    epochs: 5
    batch_size: 64
    expected_accuracy: 0.97
    priority: medium

  # Sequence model (if time permits)
  # - name: IMDB_Sentiment_LSTM
  #   description: "LSTM for sentiment analysis"
  #   dataset: imdb
  #   model_type: lstm
  #   epochs: 3
  #   batch_size: 32
  #   expected_accuracy: 0.85
  #   priority: low

# Metrics to collect
metrics:
  code_quality:
    - lines_of_code
    - setup_complexity
    - code_readability_score
  
  performance:
    - training_time_seconds
    - inference_time_ms
    - compilation_time_seconds
    - peak_memory_mb
  
  model_quality:
    - model_accuracy
    - model_size_mb
    - parameters_count
    - error_rate

# Comparison dimensions
comparisons:
  primary:
    - lines_of_code  # Most important: code reduction
    - development_time_seconds  # Developer productivity
    - model_accuracy  # Model quality
  
  secondary:
    - training_time_seconds  # Runtime performance
    - inference_time_ms  # Deployment performance
    - setup_complexity  # Ease of use
    - code_readability_score  # Maintainability
  
  tertiary:
    - model_size_mb  # Deployment size
    - compilation_time_seconds  # DSL overhead
    - peak_memory_mb  # Resource usage

# Visualization settings
visualizations:
  enabled: true
  formats:
    - png
    - svg
  dpi: 300
  style: seaborn-v0_8-darkgrid
  
  plots:
    - type: bar
      metric: lines_of_code
      title: "Lines of Code Comparison"
      lower_is_better: true
    
    - type: bar
      metric: training_time_seconds
      title: "Training Time Comparison"
      lower_is_better: true
    
    - type: bar
      metric: inference_time_ms
      title: "Inference Latency Comparison"
      lower_is_better: true
    
    - type: bar
      metric: model_accuracy
      title: "Model Accuracy Comparison"
      lower_is_better: false
    
    - type: radar
      metrics:
        - lines_of_code
        - training_time_seconds
        - model_accuracy
        - code_readability_score
      title: "Multi-Dimensional Framework Comparison"
    
    - type: heatmap
      title: "Framework Performance Heatmap"
      normalize: true

# Report generation settings
report:
  format: 
    - html
    - markdown
    - json
  
  include_sections:
    - executive_summary
    - comparison_tables
    - visualizations
    - reproducibility_guide
    - raw_data
  
  branding:
    title: "Neural DSL Performance Benchmarks"
    subtitle: "Comprehensive Framework Comparison"
    logo: assets/logo.png
    colors:
      primary: "#FF6B6B"
      secondary: "#4ECDC4"

# Reproducibility settings
reproducibility:
  seed: 42
  deterministic: true
  include_environment: true
  include_git_hash: true
  include_hardware_info: true

# Publishing settings
publishing:
  website_dir: website/docs
  assets_dir: website/docs/assets/benchmarks
  
  update_files:
    - benchmarks.md
    - benchmark_summary.md
  
  generate_artifacts:
    - comparison_table.csv
    - summary_statistics.json
    - reproducibility_script.py

# Advanced options
advanced:
  profile_memory: true
  profile_cpu: false
  profile_gpu: false
  enable_tensorboard: false
  save_checkpoints: false
  early_stopping: false
  
  # Framework-specific options
  framework_options:
    neural_dsl:
      backend: tensorflow  # tensorflow, pytorch, onnx
      optimize_graph: true
      enable_mixed_precision: false
    
    keras:
      use_legacy_optimizer: false
    
    pytorch_lightning:
      precision: 32
      accelerator: auto

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural DSL on Google Colab - Complete Guide\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Lemniscate-SHA-256/Neural/blob/main/neural/cloud/examples/neural_colab_example.ipynb)\n",
    "\n",
    "This notebook provides a comprehensive guide to using Neural DSL in Google Colab with GPU/TPU support, cloud optimizations, and advanced features.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [GPU/TPU Configuration](#gpu-tpu)\n",
    "3. [Quick Start](#quick-start)\n",
    "4. [Advanced Model Building](#advanced-models)\n",
    "5. [Hyperparameter Optimization](#hpo)\n",
    "6. [Model Visualization and Debugging](#visualization)\n",
    "7. [Production Deployment](#deployment)\n",
    "8. [Best Practices](#best-practices)\n",
    "9. [Troubleshooting](#troubleshooting)\n",
    "10. [Cleanup](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation <a id='setup'></a>\n",
    "\n",
    "Install Neural DSL and required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Neural DSL with full dependencies\n",
    "!pip install -q git+https://github.com/Lemniscate-SHA-256/Neural.git\n",
    "\n",
    "# Install cloud-specific dependencies\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print(\"‚úì Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU/TPU Configuration <a id='gpu-tpu'></a>\n",
    "\n",
    "Check and configure GPU/TPU availability.\n",
    "\n",
    "**Note:** To enable GPU in Colab, go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Check TensorFlow GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Cloud Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural.cloud.cloud_execution import CloudExecutor\n",
    "import json\n",
    "\n",
    "# Initialize with Colab-optimized settings\n",
    "executor = CloudExecutor(\n",
    "    timeout=900,  # 15 minutes for longer training runs\n",
    "    retry_attempts=3\n",
    ")\n",
    "\n",
    "# Display environment information\n",
    "env_info = executor.get_environment_info()\n",
    "print(\"Environment Configuration:\")\n",
    "print(json.dumps(env_info, indent=2, default=str))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Environment: {executor.environment}\")\n",
    "print(f\"GPU Available: {executor.is_gpu_available}\")\n",
    "print(f\"Optimization Level: {executor.optimization_level}/3\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Start <a id='quick-start'></a>\n",
    "\n",
    "Build and train your first model in 3 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define model\n",
    "simple_model = \"\"\"\n",
    "network SimpleCNN {\n",
    "    input: (28, 28, 1)\n",
    "    layers:\n",
    "        Conv2D(32, (3, 3), \"relu\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Flatten()\n",
    "        Dense(128, \"relu\")\n",
    "        Dense(10, \"softmax\")\n",
    "    loss: \"categorical_crossentropy\"\n",
    "    optimizer: Adam(learning_rate=0.001)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Compile\n",
    "model_path = executor.compile_model(simple_model, backend='tensorflow')\n",
    "print(f\"‚úì Model compiled to: {model_path}\")\n",
    "\n",
    "# Step 3: Train\n",
    "results = executor.run_model(model_path, dataset='MNIST', epochs=2, batch_size=128)\n",
    "if results['success']:\n",
    "    print(\"‚úì Training completed!\")\n",
    "else:\n",
    "    print(f\"‚úó Training failed: {results['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Model Building <a id='advanced-models'></a>\n",
    "\n",
    "### 4.1 CIFAR-10 CNN with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_model = \"\"\"\n",
    "network Cifar10Advanced {\n",
    "    input: (32, 32, 3)\n",
    "    layers:\n",
    "        # First convolutional block\n",
    "        Conv2D(64, (3, 3), \"relu\", padding=\"same\")\n",
    "        BatchNormalization()\n",
    "        Conv2D(64, (3, 3), \"relu\", padding=\"same\")\n",
    "        BatchNormalization()\n",
    "        MaxPooling2D((2, 2))\n",
    "        Dropout(0.25)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(128, (3, 3), \"relu\", padding=\"same\")\n",
    "        BatchNormalization()\n",
    "        Conv2D(128, (3, 3), \"relu\", padding=\"same\")\n",
    "        BatchNormalization()\n",
    "        MaxPooling2D((2, 2))\n",
    "        Dropout(0.25)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        Conv2D(256, (3, 3), \"relu\", padding=\"same\")\n",
    "        BatchNormalization()\n",
    "        Conv2D(256, (3, 3), \"relu\", padding=\"same\")\n",
    "        BatchNormalization()\n",
    "        GlobalAveragePooling2D()\n",
    "        Dropout(0.25)\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(512, \"relu\")\n",
    "        BatchNormalization()\n",
    "        Dropout(0.5)\n",
    "        Dense(10, \"softmax\")\n",
    "    loss: \"categorical_crossentropy\"\n",
    "    optimizer: Adam(learning_rate=0.001)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cifar_path = executor.compile_model(cifar_model, backend='tensorflow', validate=True)\n",
    "    print(f\"‚úì CIFAR-10 model compiled: {cifar_path}\")\n",
    "    \n",
    "    # Train with longer epochs\n",
    "    cifar_results = executor.run_model(\n",
    "        cifar_path,\n",
    "        dataset='CIFAR10',\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        timeout=600\n",
    "    )\n",
    "    \n",
    "    if cifar_results['success']:\n",
    "        print(\"‚úì CIFAR-10 training completed successfully!\")\n",
    "        print(\"\\nTraining output (last 500 chars):\")\n",
    "        print(cifar_results['stdout'][-500:])\n",
    "    else:\n",
    "        print(f\"‚úó Training failed: {cifar_results['error']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Backend Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the same model to multiple backends\n",
    "comparison_model = \"\"\"\n",
    "network ComparisonNet {\n",
    "    input: (28, 28, 1)\n",
    "    layers:\n",
    "        Conv2D(32, (3, 3), \"relu\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Conv2D(64, (3, 3), \"relu\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Flatten()\n",
    "        Dense(128, \"relu\")\n",
    "        Dense(10, \"softmax\")\n",
    "    loss: \"categorical_crossentropy\"\n",
    "    optimizer: Adam(learning_rate=0.001)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "backends = ['tensorflow', 'pytorch']\n",
    "backend_models = {}\n",
    "\n",
    "print(\"Compiling to multiple backends...\\n\")\n",
    "for backend in backends:\n",
    "    try:\n",
    "        path = executor.compile_model(comparison_model, backend=backend)\n",
    "        backend_models[backend] = path\n",
    "        print(f\"‚úì {backend.upper()}: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {backend.upper()} failed: {e}\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully compiled to {len(backend_models)}/{len(backends)} backends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Optimization <a id='hpo'></a>\n",
    "\n",
    "Use Neural DSL's HPO features to automatically find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_model = \"\"\"\n",
    "network HPOModel {\n",
    "    input: (32, 32, 3)\n",
    "    layers:\n",
    "        Conv2D(HPO(choice(32, 64, 128)), (3, 3), \"relu\", padding=\"same\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Dropout(HPO(range(0.1, 0.5, step=0.1)))\n",
    "        \n",
    "        Conv2D(HPO(choice(64, 128, 256)), (3, 3), \"relu\", padding=\"same\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Dropout(HPO(range(0.1, 0.5, step=0.1)))\n",
    "        \n",
    "        Flatten()\n",
    "        Dense(HPO(choice(128, 256, 512)), \"relu\")\n",
    "        Dropout(0.5)\n",
    "        Dense(10, \"softmax\")\n",
    "    loss: \"categorical_crossentropy\"\n",
    "    optimizer: Adam(learning_rate=HPO(log_range(0.0001, 0.01)))\n",
    "    train {\n",
    "        epochs: 10\n",
    "        batch_size: HPO(choice(32, 64, 128))\n",
    "        search_method: \"bayesian\"\n",
    "        max_trials: 20\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Setting up hyperparameter optimization...\")\n",
    "    hpo_path = executor.compile_model(hpo_model, backend='tensorflow')\n",
    "    print(f\"‚úì HPO model compiled: {hpo_path}\")\n",
    "    \n",
    "    print(\"\\nStarting hyperparameter search (this may take several minutes)...\")\n",
    "    hpo_results = executor.run_model(\n",
    "        hpo_path,\n",
    "        dataset='CIFAR10',\n",
    "        timeout=1800  # 30 minutes\n",
    "    )\n",
    "    \n",
    "    if hpo_results['success']:\n",
    "        print(\"‚úì Hyperparameter optimization completed!\")\n",
    "        print(\"\\nResults (last 1000 chars):\")\n",
    "        print(hpo_results['stdout'][-1000:])\n",
    "    else:\n",
    "        print(f\"‚úó HPO failed: {hpo_results['error']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó HPO error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Visualization and Debugging <a id='visualization'></a>\n",
    "\n",
    "### 6.1 Visualize Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Visualize the CIFAR model\n",
    "try:\n",
    "    viz_path = executor.visualize_model(cifar_model, output_format='png')\n",
    "    print(f\"‚úì Visualization saved to: {viz_path}\")\n",
    "    display(Image(filename=viz_path))\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Interactive Debugging with NeuralDbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Start debugging dashboard\n",
    "try:\n",
    "    print(\"Starting NeuralDbg dashboard...\")\n",
    "    dashboard = executor.start_debug_dashboard(\n",
    "        cifar_model,\n",
    "        backend='tensorflow',\n",
    "        setup_tunnel=True,\n",
    "        port=8050\n",
    "    )\n",
    "    \n",
    "    if dashboard['status'] == 'running':\n",
    "        print(f\"‚úì Dashboard is running!\")\n",
    "        print(f\"Process ID: {dashboard['process_id']}\")\n",
    "        print(f\"Local URL: {dashboard['dashboard_url']}\")\n",
    "        print(f\"Public URL: {dashboard['tunnel_url']}\")\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <div style='padding: 20px; background-color: #f0f0f0; border-radius: 5px; margin: 10px 0;'>\n",
    "            <h3>üîç NeuralDbg Dashboard</h3>\n",
    "            <p><a href='{dashboard['tunnel_url']}' target='_blank' style='color: #0066cc; font-size: 16px;'>\n",
    "                Click here to open the dashboard\n",
    "            </a></p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    else:\n",
    "        print(f\"‚úó Dashboard failed: {dashboard.get('error', 'Unknown error')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Could not start dashboard: {e}\")\n",
    "    print(\"Note: ngrok tunnel requires authentication. Set up your ngrok auth token if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 No-Code Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start No-Code visual interface\n",
    "try:\n",
    "    print(\"Starting No-Code interface...\")\n",
    "    nocode = executor.start_nocode_interface(\n",
    "        port=8051,\n",
    "        setup_tunnel=True\n",
    "    )\n",
    "    \n",
    "    if nocode['status'] == 'running':\n",
    "        print(f\"‚úì No-Code interface is running!\")\n",
    "        print(f\"Process ID: {nocode['process_id']}\")\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <div style='padding: 20px; background-color: #e8f4f8; border-radius: 5px; margin: 10px 0;'>\n",
    "            <h3>üé® Neural No-Code Interface</h3>\n",
    "            <p><a href='{nocode['tunnel_url']}' target='_blank' style='color: #0066cc; font-size: 16px;'>\n",
    "                Click here to open the visual builder\n",
    "            </a></p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    else:\n",
    "        print(f\"‚úó No-Code interface failed: {nocode.get('error', 'Unknown error')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Could not start No-Code interface: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Deployment <a id='deployment'></a>\n",
    "\n",
    "Export models for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save model for deployment\n",
    "import os\n",
    "\n",
    "deployment_model = \"\"\"\n",
    "network ProductionModel {\n",
    "    input: (224, 224, 3)\n",
    "    layers:\n",
    "        Conv2D(64, (3, 3), \"relu\", padding=\"same\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Conv2D(128, (3, 3), \"relu\", padding=\"same\")\n",
    "        MaxPooling2D((2, 2))\n",
    "        Conv2D(256, (3, 3), \"relu\", padding=\"same\")\n",
    "        GlobalAveragePooling2D()\n",
    "        Dense(1000, \"softmax\")\n",
    "    loss: \"categorical_crossentropy\"\n",
    "    optimizer: Adam(learning_rate=0.001)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile for production\n",
    "production_path = executor.compile_model(\n",
    "    deployment_model,\n",
    "    backend='tensorflow',\n",
    "    output_file='/content/production_model.py'\n",
    ")\n",
    "\n",
    "print(f\"‚úì Production model saved to: {production_path}\")\n",
    "print(f\"File size: {os.path.getsize(production_path)} bytes\")\n",
    "\n",
    "# Download the model\n",
    "print(\"\\nTo download the model:\")\n",
    "print(\"1. Click on the folder icon on the left\")\n",
    "print(\"2. Right-click on 'production_model.py'\")\n",
    "print(\"3. Select 'Download'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices <a id='best-practices'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\n=== Neural DSL on Colab: Best Practices ===\\n\n",
    "1. ‚úì GPU Usage:\n",
    "   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "   - Check availability before training\n",
    "   - Use batch normalization for faster convergence\n",
    "\n",
    "2. ‚úì Memory Management:\n",
    "   - Use appropriate batch sizes (64-128 for GPU)\n",
    "   - Enable gradient checkpointing for large models\n",
    "   - Clear GPU memory: tf.keras.backend.clear_session()\n",
    "\n",
    "3. ‚úì Training Efficiency:\n",
    "   - Use mixed precision training on compatible GPUs\n",
    "   - Leverage data augmentation\n",
    "   - Save checkpoints regularly\n",
    "\n",
    "4. ‚úì Error Handling:\n",
    "   - Always use try-except blocks\n",
    "   - Set appropriate timeouts\n",
    "   - Check model outputs before training\n",
    "\n",
    "5. ‚úì Cloud Optimizations:\n",
    "   - Neural DSL automatically applies Colab-specific optimizations\n",
    "   - GPU memory growth is enabled by default\n",
    "   - Retry logic handles transient failures\n",
    "\n",
    "6. ‚úì Debugging:\n",
    "   - Use NeuralDbg dashboard for real-time monitoring\n",
    "   - Enable verbose logging when needed\n",
    "   - Validate models before training\n",
    "\n",
    "7. ‚úì Cleanup:\n",
    "   - Always run executor.cleanup() at the end\n",
    "   - Close ngrok tunnels when done\n",
    "   - Save important outputs to Google Drive\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Troubleshooting <a id='troubleshooting'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic tests\n",
    "print(\"Running diagnostic tests...\\n\")\n",
    "\n",
    "# Test 1: Environment detection\n",
    "print(\"1. Environment Detection:\")\n",
    "print(f\"   Detected: {executor.environment}\")\n",
    "print(f\"   Expected: colab\")\n",
    "print(f\"   Status: {'‚úì PASS' if executor.environment == 'colab' else '‚úó FAIL'}\\n\")\n",
    "\n",
    "# Test 2: GPU availability\n",
    "print(\"2. GPU Availability:\")\n",
    "print(f\"   GPU Available: {executor.is_gpu_available}\")\n",
    "print(f\"   Optimization Level: {executor.optimization_level}\\n\")\n",
    "\n",
    "# Test 3: Model compilation\n",
    "print(\"3. Model Compilation Test:\")\n",
    "test_model = \"\"\"\n",
    "network TestNet {\n",
    "    input: (28, 28, 1)\n",
    "    layers:\n",
    "        Flatten()\n",
    "        Dense(10, \"softmax\")\n",
    "    loss: \"categorical_crossentropy\"\n",
    "    optimizer: Adam()\n",
    "}\n",
    "\"\"\"\n",
    "try:\n",
    "    test_path = executor.compile_model(test_model, backend='tensorflow')\n",
    "    print(f\"   Status: ‚úì PASS\")\n",
    "    print(f\"   Output: {test_path}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   Status: ‚úó FAIL\")\n",
    "    print(f\"   Error: {e}\\n\")\n",
    "\n",
    "# Test 4: Error handling\n",
    "print(\"4. Error Handling Test:\")\n",
    "try:\n",
    "    executor.compile_model(\"\", backend='tensorflow')\n",
    "    print(f\"   Status: ‚úó FAIL (should have raised error)\\n\")\n",
    "except Exception:\n",
    "    print(f\"   Status: ‚úì PASS (correctly caught empty DSL)\\n\")\n",
    "\n",
    "print(\"Diagnostic tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup <a id='cleanup'></a>\n",
    "\n",
    "Clean up all resources before closing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive cleanup\n",
    "print(\"Starting cleanup...\\n\")\n",
    "\n",
    "# 1. Clean up executor resources\n",
    "executor.cleanup()\n",
    "print(\"‚úì Executor resources cleaned\")\n",
    "\n",
    "# 2. Clear TensorFlow session\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"‚úì TensorFlow session cleared\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 3. Clear PyTorch cache (if available)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úì PyTorch CUDA cache cleared\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n‚úì Cleanup complete!\")\n",
    "print(\"\\nThank you for using Neural DSL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. ‚úì **Setup**: Installation and configuration\n",
    "2. ‚úì **GPU/TPU**: Hardware acceleration setup\n",
    "3. ‚úì **Quick Start**: Build and train in 3 steps\n",
    "4. ‚úì **Advanced Models**: Complex architectures with regularization\n",
    "5. ‚úì **HPO**: Automatic hyperparameter optimization\n",
    "6. ‚úì **Visualization**: Model architecture and debugging tools\n",
    "7. ‚úì **Deployment**: Export for production\n",
    "8. ‚úì **Best Practices**: Tips for efficient cloud usage\n",
    "9. ‚úì **Troubleshooting**: Diagnostic tests and solutions\n",
    "10. ‚úì **Cleanup**: Proper resource management\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- üìö Read the [documentation](https://github.com/Lemniscate-SHA-256/Neural)\n",
    "- üöÄ Try more [examples](https://github.com/Lemniscate-SHA-256/Neural/tree/main/examples)\n",
    "- üí¨ Join the [community](https://github.com/Lemniscate-SHA-256/Neural/discussions)\n",
    "- üêõ Report [issues](https://github.com/Lemniscate-SHA-256/Neural/issues)\n",
    "\n",
    "Happy modeling! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

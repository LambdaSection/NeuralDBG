# Federated Learning Configuration Example
# This file demonstrates all available configuration options

# Backend framework (tensorflow or pytorch)
backend: tensorflow

# Model definition using Neural DSL format
model:
  model_data:
    input:
      shape: [784]  # Input shape (excluding batch dimension)
    
    layers:
      - type: dense
        units: 256
        activation: relu
      
      - type: dropout
        rate: 0.3
      
      - type: dense
        units: 128
        activation: relu
      
      - type: dropout
        rate: 0.2
      
      - type: output
        units: 10
        activation: softmax

# General training configuration
training:
  batch_size: 32
  learning_rate: 0.01
  evaluate_every: 5  # Evaluate every N rounds

# Federated learning configuration
federated:
  enabled: true
  
  # Scenario type: cross_device, cross_silo, hybrid, vertical
  scenario: cross_device
  
  # Cross-device specific settings
  num_devices: 100
  devices_per_round: 10
  data_heterogeneity: 0.5  # 0=IID, 1=highly non-IID
  device_availability: 0.8  # Probability a device is available
  
  # Cross-silo specific settings (alternative to cross-device)
  # num_silos: 10
  # silos_per_round: 5
  
  # Hybrid specific settings (alternative)
  # num_silos: 5
  # devices_per_silo: 20
  # silos_per_round: 3
  # devices_per_silo_per_round: 5
  
  # Training parameters
  num_rounds: 100
  local_epochs: 1  # Local training epochs per client
  
  # Aggregation strategy: fedavg, fedprox, fedadam, fedyogi, fedma, adaptive
  aggregation: fedavg
  
  # FedProx specific (if using fedprox)
  proximal_mu: 0.01
  
  # Server optimizer settings (for fedadam, fedyogi)
  server_lr: 0.01
  beta_1: 0.9
  beta_2: 0.999
  
  # Client selection
  min_available_clients: 2
  fraction_fit: 1.0  # Fraction of available clients for training
  fraction_evaluate: 1.0  # Fraction for evaluation
  
  # Differential Privacy
  privacy:
    enabled: true
    mechanism: gaussian  # gaussian, laplacian, shuffle, adaptive
    epsilon: 1.0  # Privacy budget
    delta: 1e-5  # Privacy parameter
    clip_norm: 1.0  # Gradient clipping threshold
    sensitivity: 1.0  # Sensitivity parameter
  
  # Communication Compression
  compression:
    enabled: true
    type: quantization  # quantization, sparsification, adaptive
    
    # Quantization settings
    num_bits: 8  # Bit precision (1-32)
    stochastic_quantization: false
    
    # Sparsification settings (alternative)
    # sparsity: 0.9  # Keep only 10% of weights
    # sparsification_method: topk  # topk, threshold, random
    
    # Adaptive compression (combines both)
    # target_compression: 0.5
    
    # Error feedback
    error_feedback: true
  
  # Secure Aggregation
  secure_aggregation: true
  aggregation_threshold: 2  # Minimum clients for secure aggregation
  
  # Communication Scheduling
  communication_interval: 1  # Communicate every N rounds
  max_communication_interval: 10
  adaptation_rate: 0.1
  
  # Data distribution
  data_distribution: non_iid  # iid, non_iid
  non_iid_alpha: 0.5  # Dirichlet alpha for non-IID (lower = more skewed)
  
  # Save/Load
  save_model_path: federated_model.h5
  save_metrics_path: federated_metrics.json
  checkpoint_interval: 10  # Save checkpoint every N rounds

# Advanced options
advanced:
  # Byzantine detection
  byzantine_detection: true
  byzantine_threshold: 0.5
  
  # Resource awareness
  resource_aware_selection: true
  
  # Logging
  log_level: INFO
  tensorboard_dir: runs/federated
  
  # Checkpointing
  enable_checkpoints: true
  checkpoint_dir: federated_checkpoints/

name: Benchmarks

on:
  workflow_dispatch:
  schedule:
    # Run weekly on Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  push:
    branches:
      - main
    paths:
      - 'neural/benchmarks/**'
      - '.github/workflows/benchmarks.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[full]"
        pip install pytest pytest-cov
    
    - name: Run unit tests
      run: |
        pytest tests/benchmarks/ -v --cov=neural.benchmarks --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: benchmarks
    
    - name: Run quick benchmark
      run: |
        python neural/benchmarks/quick_start.py
      continue-on-error: true
    
    - name: Archive benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results-${{ matrix.python-version }}
        path: |
          quick_start_results/
          quick_start_reports/
        retention-days: 30

  full-benchmark:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[full]"
        # Install optional frameworks
        pip install pytorch-lightning || echo "PyTorch Lightning installation failed"
        pip install fastai || echo "Fast.ai installation failed"
    
    - name: Run full benchmarks
      run: |
        python neural/benchmarks/run_benchmarks.py --epochs 5
      continue-on-error: true
    
    - name: Archive full benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: full-benchmark-results
        path: |
          benchmark_results/
          benchmark_reports/
        retention-days: 90
    
    - name: Publish to GitHub Pages
      if: github.ref == 'refs/heads/main' && github.event_name == 'schedule'
      run: |
        # Find latest report
        LATEST_REPORT=$(ls -td benchmark_reports/neural_dsl_benchmark_* | head -1)
        
        # Setup git
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
        # Clone gh-pages branch
        git clone --branch gh-pages https://github.com/${{ github.repository }} gh-pages || \
          git clone https://github.com/${{ github.repository }} gh-pages && \
          cd gh-pages && \
          git checkout -b gh-pages && \
          cd ..
        
        # Publish results
        python neural/benchmarks/publish_to_website.py \
          "$LATEST_REPORT" \
          --github-pages gh-pages/
        
        # Commit and push
        cd gh-pages
        git add .
        git commit -m "Update benchmark results [automated]" || echo "No changes to commit"
        git push origin gh-pages || echo "Push failed"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

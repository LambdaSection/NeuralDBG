# AntiGravity Rules â€” NeuralDBG / OpenQuant Roadmap

> **Workflow**: On travaille sur **un seul projet Ã  la fois**. Finir A avant de passer Ã  B, ou vice versa. Pas de dÃ©veloppement en parallÃ¨le.

---

## ğŸ”· Projet A â€” Transformer Probabiliste pour SÃ©ries Temporelles

**Orientation**: OpenQuant

### Objectif
ModÃ©liser une **distribution conditionnelle** :  
\( P(Y_{t+1} \mid X_{1:t}) \)  

Pas une valeur. Une **incertitude exploitable**.

### Architecture V1 (minimaliste mais sÃ©rieuse)
- Input embedding (features + time encoding)
- Positional encoding
- Transformer Encoder
- Head probabiliste :
  - Î¼ (mean)
  - Ïƒ (std)
  - Optionnel : mixture logits

### Dataset
- **Commencer synthÃ©tique** (sinusoÃ¯de bruitÃ©e)
- Si Ã§a ne marche pas sur du propre, Ã§a ne marchera pas sur du marchÃ©
- Plus tard : Crypto OHLCV, Forex

### Loss
Negative Log Likelihood (Gaussian) :
```
L = (1/2) log(ÏƒÂ²) + (y âˆ’ Î¼)Â² / (2ÏƒÂ²)
```
Ã‡a force le modÃ¨le Ã  calibrer son incertitude.

### Extensions futures
- Multi-head temporal attention
- Multi-horizon forecasting
- Calibration testing (Expected Calibration Error)
- Backtesting avec gestion du risque

---

## ğŸ”¶ Projet B â€” Adaptive Gradient Architecture

**Orientation**: Neural / NeuralDBG

### Objectif
On ne prÃ©dit rien. On **observe et corrige** la dynamique interne.

CrÃ©er une couche qui :
- Mesure la norme des gradients layer-wise
- DÃ©tecte une dÃ©croissance anormale
- Applique une correction adaptative

### Concept V1 (simple et puissant)
Ã€ chaque backward pass :
1. Calculer â€–âˆ‡Wâ€–
2. Maintenir une moyenne mobile (EMA)
3. Si gradient < seuil dynamique â†’ rescale

### Structure cible
```python
class AdaptiveGradientWrapper(nn.Module):
    def __init__(self, module):
        super().__init__()
        self.module = module
        self.grad_ema = None
```
On encapsule n'importe quelle couche.

### MÃ©canisme
Si â€–âˆ‡Wâ€– < Î± Â· EMA  
Alors âˆ‡W â† Î² Â· âˆ‡W  (avec Î² > 1)

### ExpÃ©riences Ã  mener
- Tester sur : Deep MLP 50 couches, RNN long sequence, Transformer profond
- Comparer : convergence speed, stabilitÃ©, distribution des gradients

---

## Artifacts
Maintenir les artefacts gÃ©nÃ©rÃ©s sous `./.antigravity/artifacts/` (reports, plots, checkpoints, etc.).
